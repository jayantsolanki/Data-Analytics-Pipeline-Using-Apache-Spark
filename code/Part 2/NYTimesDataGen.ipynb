{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-weight:bold\">Jayant Solanki</span>,\n",
    "<span style=\"color:red;font-weight:bold\">Anant Gupta</span>\n",
    "<hr/>\n",
    "## <span style=\"float:left\">Lab 3</span>\n",
    "### <span style=\"float:right\">DATA ANALYTICS PIPELINE USING APACHE SPARK</span>\n",
    "#### <span style=\"float:right\">NY Times article collection and creation of the csv file</span>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Background: The nytimes api doesn't returns full article body, just the snippet of it each returned\n",
    "# so, I have to write the code to fetch the weburl for each article, then perfrom further requests to those url and \n",
    "#fetch the story body of each article\n",
    "import requests # for performing html request to nytimes API\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import lxml.html as html # for scrapping the content of the article URL of nytimes\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_word_list = stopwords.words('english')\n",
    "#  to quickly test if a word is not a stop word, use a set:\n",
    "stop_word_set = set(stop_word_list)\n",
    "# Each API keys can provide 10000 response hits\n",
    "# topic = [\"baseball\", \"politics\",\"market\",\"fashion\"]#topic to be looked for\n",
    "# topic = [\"soccer\", \"election\",\"economy\",\"movies\"]#topic to be looked for\n",
    "# topic = [\"sport\", \"trump\",\"finance\",\"music\"]#topic to be looked for\n",
    "# topic = [\"hockey\", \"north korea\",\"business\",\"entertainment\"]#topic to be looked for\n",
    "# topic = [\"basketball\", \"south china sea\",\"money\",\"dance\"]#topic to be looked for\n",
    "topic = [\"basketball\", \"south china sea\",\"money\",\"celebrity\"]#topic to be looked for\n",
    "\n",
    "apikeyIndex=0;\n",
    "apikey = [\"ddfe36f13f354e348d9cea28e2b27001\", \"2c94bb3581cf469fb33321a9e3bbac38\",\"f4918bd3047241889c282af42bd2128a\",\"ddfe36f13f354e348d9cea28e2b27001\"]#add more apikeys here\n",
    "fl = \"snippet,web_url\"#selective attributes of json response\n",
    "pageNo = \"0\"#initial page is 0, articles fetched using api are grouped in 10 per page starting 0 and upto page 100\n",
    "dateRange = [\"20180501\", \"20180502\",\"20180503\", \"20180504\",\"20180505\", \"20180506\",\"20180507\", \"20180508\"]\n",
    "category = [\"sports\", \"politics\",\"business\",\"entertainment\"]\n",
    "# dateRange = [\"20180321\", \"20180322\", \"20180323\", \"20180324\", \"20180325\", \"20180326\", \"20180327\", \"20180328\"]# can be changed t any period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function parse the json response from nytimes and create new dictionary using two attributes only\n",
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    news = []\n",
    "    fetch = articles['response']['docs']\n",
    "    for i in range(0,len(fetch)):\n",
    "        dic = {}\n",
    "#         print(fetch[i])\n",
    "        dic['web_url'] = fetch[i]['web_url']\n",
    "        if fetch[i]['snippet'] is not None:\n",
    "            dic['snippet'] = fetch[i]['snippet']\n",
    "#         dic['url'] = i['web_url']\n",
    "        news.append(dic)\n",
    "    return(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function perfrom request to nytimes api using the paramters passed and returns the parseed responsed to the caller function\n",
    "def get_articles(topic, begin_date, end_date, fl, apikey, apikeyIndex):\n",
    "    all_articles = []#stores all articles for a particular day\n",
    "    page = 0\n",
    "    while(page<100):\n",
    "        sleep(1)\n",
    "#     for page in range(0,100): #NYT limits pages to first 100 pages starting page 0, each page has 10 articles max\n",
    "        try:\n",
    "            \n",
    "            url = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\"+topic+\"&begin_date=\"+begin_date+\"&end_date=\"+end_date+\"&fl=\"+fl+\"&page=\"+str(page)+\"&api-key=\"+apikey[apikeyIndex]\n",
    "            print(url)\n",
    "            requestArticles = requests.get(url)\n",
    "            data = requestArticles.json()\n",
    "            if len(data[\"response\"][\"docs\"])>0:\n",
    "                all_articles.append(parse_articles(data))\n",
    "#                 print(data)\n",
    "            else:# checks if further pages have no articles to show, if yes then break the loop and return the fetched articles\n",
    "                print(parse_articles(data))\n",
    "                break\n",
    "        except:\n",
    "            if(data['message']=='API rate limit exceeded'):\n",
    "                print(\"You called the api way to fast, Dude, trying again\")\n",
    "                apikeyIndex= apikeyIndex+1\n",
    "            else:\n",
    "                print(data)\n",
    "#             page = page - 1\n",
    "            sleep(1)\n",
    "            continue#try again\n",
    "        print(\"Page: \"+str(page))\n",
    "#         break\n",
    "        page=page+1\n",
    "    return(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddfe36f13f354e348d9cea28e2b27001\n",
      "Fetching articles for Data period: 05-01-2018 - 05-02-2018\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180501&end_date=20180502&fl=snippet,web_url&page=0&api-key=ddfe36f13f354e348d9cea28e2b27001\n",
      "You called the api way to fast, Dude, trying again\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180501&end_date=20180502&fl=snippet,web_url&page=0&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180501&end_date=20180502&fl=snippet,web_url&page=1&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "[]\n",
      "processing full articles\n",
      "Fetching articles for Data period: 05-02-2018 - 05-03-2018\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180502&end_date=20180503&fl=snippet,web_url&page=0&api-key=ddfe36f13f354e348d9cea28e2b27001\n",
      "You called the api way to fast, Dude, trying again\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180502&end_date=20180503&fl=snippet,web_url&page=0&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180502&end_date=20180503&fl=snippet,web_url&page=1&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "[]\n",
      "processing full articles\n",
      "Fetching articles for Data period: 05-03-2018 - 05-04-2018\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180503&end_date=20180504&fl=snippet,web_url&page=0&api-key=ddfe36f13f354e348d9cea28e2b27001\n",
      "You called the api way to fast, Dude, trying again\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180503&end_date=20180504&fl=snippet,web_url&page=0&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180503&end_date=20180504&fl=snippet,web_url&page=1&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 1\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180503&end_date=20180504&fl=snippet,web_url&page=2&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "[]\n",
      "processing full articles\n",
      "Fetching articles for Data period: 05-04-2018 - 05-05-2018\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180504&end_date=20180505&fl=snippet,web_url&page=0&api-key=ddfe36f13f354e348d9cea28e2b27001\n",
      "You called the api way to fast, Dude, trying again\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180504&end_date=20180505&fl=snippet,web_url&page=0&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180504&end_date=20180505&fl=snippet,web_url&page=1&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 1\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180504&end_date=20180505&fl=snippet,web_url&page=2&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "[]\n",
      "processing full articles\n",
      "Fetching articles for Data period: 05-05-2018 - 05-06-2018\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180505&end_date=20180506&fl=snippet,web_url&page=0&api-key=ddfe36f13f354e348d9cea28e2b27001\n",
      "You called the api way to fast, Dude, trying again\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180505&end_date=20180506&fl=snippet,web_url&page=0&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180505&end_date=20180506&fl=snippet,web_url&page=1&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "[]\n",
      "processing full articles\n",
      "Fetching articles for Data period: 05-06-2018 - 05-07-2018\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180506&end_date=20180507&fl=snippet,web_url&page=0&api-key=ddfe36f13f354e348d9cea28e2b27001\n",
      "You called the api way to fast, Dude, trying again\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180506&end_date=20180507&fl=snippet,web_url&page=0&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180506&end_date=20180507&fl=snippet,web_url&page=1&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "[]\n",
      "processing full articles\n",
      "Fetching articles for Data period: 05-07-2018 - 05-08-2018\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180507&end_date=20180508&fl=snippet,web_url&page=0&api-key=ddfe36f13f354e348d9cea28e2b27001\n",
      "You called the api way to fast, Dude, trying again\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180507&end_date=20180508&fl=snippet,web_url&page=0&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "Page: 0\n",
      "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=celebrity&begin_date=20180507&end_date=20180508&fl=snippet,web_url&page=1&api-key=2c94bb3581cf469fb33321a9e3bbac38\n",
      "[]\n",
      "processing full articles\n",
      "Fetching Completed\n"
     ]
    }
   ],
   "source": [
    "with open(\"textcorpus/articles.csv\", 'a') as outfile:#for storing full articles in the csv\n",
    "    for labels in range(3,4):\n",
    "        print(apikey[apikeyIndex])\n",
    "        processArticles = []\n",
    "#         if not os.path.exists(\"textcorpus\"):\n",
    "#             os.makedirs(\"textcorpus\")\n",
    "#         if not os.path.exists(\"textcorpus/\"+category[index]):\n",
    "#             os.makedirs(\"textcorpus/\"+category[index])\n",
    "\n",
    "        for i in range(0,len(dateRange)-1):\n",
    "            datetimeobject = datetime.strptime(dateRange[i],'%Y%m%d')\n",
    "            beginDate = datetimeobject.strftime('%m-%d-%Y')\n",
    "            datetimeobject = datetime.strptime(dateRange[i+1],'%Y%m%d')\n",
    "            endDate = datetimeobject.strftime('%m-%d-%Y')\n",
    "            print(\"Fetching articles for Data period: \" + beginDate + \" - \"+ endDate)\n",
    "            processArticles = get_articles(topic[labels], dateRange[i], dateRange[i+1],fl, apikey, apikeyIndex)\n",
    "            if(len(processArticles)>0):\n",
    "        #         try:\n",
    "        #             dataToWrite = processArticles\n",
    "        # #             print(dataToWrite)\n",
    "        #         except:\n",
    "        #             print(len(processArticles))\n",
    "        #             print(processArticles)\n",
    "        #             print(processArticles[0])\n",
    "        #             break\n",
    "    #                 print(\"processing snippets\")\n",
    "    #                 count=0\n",
    "    #                 for item in processArticles:\n",
    "    #                     page=0\n",
    "    #                     for articles in item:\n",
    "    #                         count=count+1\n",
    "    #                         page=page+1\n",
    "    #                         articlesCount=\"page\"+str(page)+\"articles\"+str(count)\n",
    "    #                         with open(\"textcorpus/\"+category[index]+\"/\"+topic[index]+articlesCount+dateRange[i]+\".txt\", 'w') as outfile:#used for storing snippets\n",
    "\n",
    "    #         #                     print(articles)\n",
    "    #                             outfile.write(articles[\"snippet\"])\n",
    "    #                             outfile.write(\"\\n\")\n",
    "                    print(\"processing full articles\")\n",
    "                    count=0\n",
    "                    for item in processArticles:\n",
    "                        page=0\n",
    "                        for articles in item:\n",
    "                            count=count+1\n",
    "                            page=page+1\n",
    "                            articlesCount=\"page\"+str(page)+\"articles\"+str(count)\n",
    "                            fullpage = requests.get(articles[\"web_url\"])\n",
    "                            htmlbody = html.fromstring(requests.get(articles[\"web_url\"]).content)\n",
    "                            output = \"\".join(htmlbody.xpath('//p[contains(@class,\"story-body-text\")]//text()'))#scrapper\n",
    "                            output = output.join(htmlbody.xpath('//div[contains(@class,\"StoryBodyCompanionColumn\")]//text()'))#scrapper\n",
    "        #                     if(len(output)<100):\n",
    "        #                         break\n",
    "        #                     print(len(output))\n",
    "        #                     print(output+\"\\n\\n\")\n",
    "        #                     output = output.decode('utf8').encode('latin1').decode('utf8')\n",
    "                            output = str(output.encode(\"ascii\", \"ignore\"))#since the body has lots of escape characters, I have to convert\n",
    "            # utf-8 response into ascii using ignore flag to bypass those escape characters\n",
    "                            # do filtering of useless characters here\n",
    "                            output = output.strip(',')\n",
    "                            output=output.strip('\"')\n",
    "                            output=output.strip('.')\n",
    "                            output=output.strip('“')\n",
    "                            output=output.strip('$')\n",
    "                            output=output.strip('?')\n",
    "                            output=output.strip(')')\n",
    "                            output=output.strip('(')\n",
    "                            output=output.strip(' ')\n",
    "                            output   = re.sub(r'\\$\\w*', '', output)\n",
    "                            output   = re.sub(r'http?:.*$', '', output)\n",
    "                            output   = re.sub(r'https?:.*$', '', output)\n",
    "                            output   = re.sub(r'pic?.*\\/\\w*', '', output)\n",
    "                            output   = re.sub(r'[' + string.punctuation + ']+', ' ', output)  # Remove puncutations like 's\n",
    "                            output=str.replace(output,'\\'s','')\n",
    "                            output=str.replace(output,'\\\\','')\n",
    "                            output=str.replace(output,'s\\'','')\n",
    "                            output=str.replace(output,',','')\n",
    "                            output = output.upper()\n",
    "                            if(len(output[2:-1])<5):\n",
    "                                          continue\n",
    "                            #appending with userfull columns\n",
    "                            output=dateRange[i]+\",\"+category[labels]+\",\"+topic[labels]+\",\"+str(page)+\",\"+output[2:-1]\n",
    "                            outfile.write(output)\n",
    "                            outfile.write(\"\\n\")\n",
    "            else:\n",
    "                print(\"Insufficient data for date: \"+beginDate+\" to save\")\n",
    "        #     break\n",
    "        # print(processArticles[0][0:-1])\n",
    "print(\"Fetching Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
