{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import time\n",
    "import pyspark\n",
    "import os\n",
    "import csv\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start a spark session\n",
    "sc =SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path='train.csv'\n",
    "test_path='test.csv'\n",
    "# Load csv file as RDD\n",
    "train_rdd = sc.textFile(train_path)\n",
    "test_rdd = sc.textFile(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked',\n",
       " '1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S',\n",
       " '2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse RDD to DF\n",
    "def parseTrain(rdd):\n",
    "\t\n",
    "\t# extract data header (first row)\n",
    "\theader = rdd.first()\n",
    "\t# remove header\n",
    "\tbody = rdd.filter(lambda r: r!=header)\n",
    "\n",
    "\tdef parseRow(row):\n",
    "\t\t# a function to parse each text row into\n",
    "\t\t# data format\n",
    "\n",
    "\t\t# remove double quote, split the text row by comma\n",
    "\t\trow_list = row.replace('\"','').split(\",\")\n",
    "\t\t# convert python list to tuple, which is \n",
    "\t\t# compatible with pyspark data structure\n",
    "\t\trow_tuple = tuple(row_list)\n",
    "\t\treturn row_tuple\n",
    "\n",
    "\trdd_parsed = body.map(parseRow)\n",
    "\n",
    "\tcolnames = header.split(\",\")\n",
    "\tcolnames.insert(3,'FirstName')\n",
    "\n",
    "\treturn rdd_parsed.toDF(colnames)\n",
    "\n",
    "def parseTest(rdd):\n",
    "\theader = rdd.first()\n",
    "\tbody = rdd.filter(lambda r: r!=header)\n",
    "\n",
    "\tdef parseRow(row):\n",
    "\t\trow_list = row.replace('\"','').split(\",\")\n",
    "\t\trow_tuple = tuple(row_list)\n",
    "\t\treturn row_tuple\n",
    "\n",
    "\trdd_parsed = body.map(parseRow)\n",
    "\n",
    "\tcolnames = header.split(\",\")\n",
    "\tcolnames.insert(2,'FirstName')\n",
    "\n",
    "\treturn rdd_parsed.toDF(colnames)\n",
    "\n",
    " \n",
    "train_df = parseTrain(train_rdd)\n",
    "test_df = parseTest(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|FirstName|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|   Braund|     Mr. Owen Harris|  male| 22|    1|    0|       A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|  Cumings| Mrs. John Bradle...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen|         Miss. Laina|female| 26|    0|    0|STON/O2. 3101282|  7.925|     |       S|\n",
      "+-----------+--------+------+---------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Train/Test Data\n",
    "\n",
    "## Add Survived column to test\n",
    "from pyspark.sql.functions import lit, col\n",
    "train_df = train_df.withColumn('Mark',lit('train'))\n",
    "test_df = (test_df.withColumn('Survived',lit(0))\n",
    "                  .withColumn('Mark',lit('test')))\n",
    "test_df = test_df[train_df.columns]\n",
    "## Append Test data to Train data\n",
    "df = train_df.unionAll(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: double (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: double (nullable = true)\n",
      " |-- Parch: double (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Mark: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert Age, SibSp, Parch, Fare to Numeric\n",
    "\n",
    "df = (df.withColumn('Age',df['Age'].cast(\"double\"))\n",
    "\t\t\t.withColumn('SibSp',df['SibSp'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Parch',df['Parch'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Fare',df['Fare'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Survived',df['Survived'].cast(\"double\"))\n",
    "\t\t\t)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing Age and Fare with the Average\n",
    "numVars = ['Survived','Age','SibSp','Parch','Fare']\n",
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    " \n",
    "missing = {var: countNull(df,var) for var in numVars}\n",
    "age_mean = df.groupBy().mean('Age').first()[0]\n",
    "fare_mean = df.groupBy().mean('Fare').first()[0]\n",
    "df = df.na.fill({'Age':age_mean,'Fare':fare_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Name|Title|\n",
      "+--------------------+-----+\n",
      "|     Mr. Owen Harris|   Mr|\n",
      "| Mrs. John Bradle...|  Mrs|\n",
      "|         Miss. Laina| Miss|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "# Extract Title from Name\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    " \n",
    "## created user defined function to extract title\n",
    "getTitle = udf(lambda name: name.split('.')[0].strip(),StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    " \n",
    "df.select('Name','Title').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|Embarked|Embarked_indexed|\n",
      "+--------+----------------+\n",
      "|       S|             0.0|\n",
      "|       C|             1.0|\n",
      "|       S|             0.0|\n",
      "+--------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index categorical variable\n",
    "catVars = ['Pclass','Sex','Embarked','Title']\n",
    " \n",
    "## index Sex variable\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "si = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_indexed')\n",
    "df_indexed = si.fit(df).transform(df).drop('Sex').withColumnRenamed('Sex_indexed','Sex')\n",
    " \n",
    "## make use of pipeline to index all categorical variables\n",
    "def indexer(df,col):\n",
    "    si = StringIndexer(inputCol = col, outputCol = col+'_indexed').fit(df)\n",
    "    return si\n",
    " \n",
    "indexers = [indexer(df,col) for col in catVars]\n",
    " \n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    " \n",
    "df_indexed.select('Embarked','Embarked_indexed').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+-----+\n",
      "| mark|label|            features|index|\n",
      "+-----+-----+--------------------+-----+\n",
      "|train|  0.0|[22.0,1.0,0.0,7.2...|  0.0|\n",
      "|train|  1.0|[38.0,1.0,0.0,71....|  1.0|\n",
      "|train|  1.0|[26.0,0.0,0.0,7.9...|  1.0|\n",
      "+-----+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to label/features format\n",
    "catVarsIndexed = [i+'_indexed' for i in catVars]\n",
    "featuresCol = numVars+catVarsIndexed\n",
    "featuresCol.remove('Survived')\n",
    "labelCol = ['Mark','Survived']\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# from pyspark.mllib.linalg import DenseVector\n",
    "row = Row('mark','label','features')\n",
    "\n",
    "df_indexed = df_indexed[labelCol+featuresCol]\n",
    "# 0-mark, 1-label, 2-features\n",
    "lf = df_indexed.rdd.map(lambda r: (row(r[0], r[1],Vectors.dense(r[2:])))).toDF()\n",
    "# index label \n",
    "lf = StringIndexer(inputCol = 'label',outputCol='index').fit(lf).transform(lf)\n",
    " \n",
    "lf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Number of Row: 636\n",
      "Validate Data Number of Row: 255\n",
      "Test Data Number of Row: 418\n"
     ]
    }
   ],
   "source": [
    "# split back train/test data\n",
    "train = lf.where(lf.mark =='train')\n",
    "test = lf.where(lf.mark =='test')\n",
    " \n",
    "# random split further to get train/validate\n",
    "train,validate = train.randomSplit([0.7,0.3],seed =121)\n",
    " \n",
    "print ('Train Data Number of Row: '+ str(train.count()))\n",
    "print ('Validate Data Number of Row: '+ str(validate.count()))\n",
    "print ('Test Data Number of Row: '+ str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ROC of Logistic Regression model is: 0.8369523688232298\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    " \n",
    "# regPara: lasso regularisation parameter (L1)\n",
    "lr = LogisticRegression(maxIter = 100, regParam = 0.05, labelCol='index').fit(train)\n",
    " \n",
    "# Evaluate model based on auc ROC(default for binary classification)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "def testModel(model, validate = validate):\n",
    "    pred = model.transform(validate)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'index')\n",
    "    return evaluator.evaluate(pred)\n",
    " \n",
    "print ('AUC ROC of Logistic Regression model is: '+str(testModel(lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LogisticRegression': 0.8369523688232297, 'DecisionTree': 0.7723828323993885, 'RandomForest': 0.8585392256749873}\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree and Random Forest\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    " \n",
    "dt = DecisionTreeClassifier(maxDepth = 3, labelCol ='index').fit(train)\n",
    "rf = RandomForestClassifier(numTrees = 100, labelCol = 'index').fit(train)\n",
    " \n",
    "models = {'LogisticRegression':lr,\n",
    "          'DecisionTree':dt,\n",
    "          'RandomForest':rf}\n",
    " \n",
    "modelPerf = {k:testModel(v) for k,v in models.items()}\n",
    " \n",
    "print (modelPerf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
