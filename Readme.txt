***
# Data-Analytics-Pipeline-Using-Apache-Spark

## Project Summary

### Using feature extraction technique along with different supervised machine learning algorithms in Spark
* We explored the Apache Spark framework and programming: sparkcontext (sc),
dataflow operations in transformations, actions, pipelines and MLib. We applied our
data analytics knowledge and machine learning skills to perform multi-class
classification of text data using Apache Spark. We build a data pipeline using data from
sources such as NY Times articles using the APIs provided by the data sources. We
assessed the accuracy of our model using a new article for each news “category” and
then compared the classification accuracy of three well-known classification algorithms,
for a given test data set.
Lastly we applied the knowledge and skills learned to solve classification problems in
other domains.
**Note**: All the data pre-processing has been done in Spark framework using PySpark.

## Documentation
***
Report and documentation can be found on this [Report](https://github.com/jayantsolanki/Data-Analytics-Pipeline-Using-Apache-Spark/blob/master/report) link

## Folder Tree
***
* [**report**](https://github.com/jayantsolanki/Data-Analytics-Pipeline-Using-Apache-Spark/blob/master/report) contains summary report detailing our implementation and results.
* [**codes**](https://github.com/jayantsolanki/Data-Analytics-Pipeline-Using-Apache-Spark/blob/master/code)  contains the source code in python and ipython notebook


## How to Run:
make sure you have pyspark library installed and minimum Spark version is 2.0
Part1: Goto to Part 1 folder and run the corresponding ipynb file
Part 2
	Goto Part 2 folder,
	For data collection run the NYTimesDataGen.ipynb
	For classification analysis run the Part-2-Spark-Text-Classifier.ipynb

